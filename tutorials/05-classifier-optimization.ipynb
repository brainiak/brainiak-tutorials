{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
    }
   },
   "source": [
    "# Classifier Optimization\n",
    "[Contributions](#contributions)\n",
    "\n",
    "In earlier notebooks, we explored classification and feature selection techniques. Throughout, we have emphasized the importance of using cross-validation to measure classifier performance and to perform feature selection. We introduced the [Pipeline package](http://scikit-learn.org/stable/modules/pipeline.html#pipeline) to help facilitate this cross-validation process. During the past exercises we didn't pay much attention to the parameters of the classifier and set them arbitrarily or based on intuition. In what follows we are going to investigate data-driven, unbiased techniques to optimize classification parameters such as choice of classifiers, cost parameters and classification penalties. We will again use the pipeline package to perform this optimization.\n",
    "\n",
    "We will be using the useful features from scikit-learn to perform cross-validation. scikit-learn also offers a simple procedure for building and automating the various steps involved in classifier optimization (e.g. data scaling => feature selection => parameter tuning). We will also explore these methods in this exercise.\n",
    "\n",
    "## Goal of this script\n",
    "1. Learn to detect circularity, and avoid it.\n",
    "2. Build a pipeline of steps to optimize classifier performance.    \n",
    "3. Use the pipeline to make an optimal classifier.  \n",
    "\n",
    "**Recap:** The localizer data we are working with ([Kim et al., 2017](https://doi.org/10.1523/JNEUROSCI.3272-16.2017)) consists of 3 runs with 5 blocks for each category. In the matlab stimulus file, the first row has the stimulus labels for the 1st, 2nd and 3rd runs of the localizer. Each run was 310 TRs.\n",
    "The 4th row contains the time when the stimulus was presented for each of the runs. The stimulus labels and their corresponding categories are as follows: 1 = Faces, 2 = Scenes, 3 = Objects\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "[1. Load the data](#load-data)  \n",
    "\n",
    "[2. Circular Inference: How to avoid double dipping](#double_dipping)  \n",
    ">[2.1 Error: Voxel selection on all the data](#example-dd-vox-sel)  \n",
    ">[2.2 Test: Verify procedure on random (permuted) labels](#example-dd-random)\n",
    "  \n",
    "[3. Cross-validation: Hyper-parameter selection and regularization](#cross_val) \n",
    ">[3.1 Grid Search](#grid_search)  \n",
    ">[3.2 Regularization: L2 vs L1](#reg)  \n",
    ">[3.3 Nested Cross-validation: Hyper-parameter selection](#nested_cross_val)   \n",
    "\n",
    "\n",
    "[4. Make a pipeline](#pipeline)  \n",
    "\n",
    "Exercises\n",
    ">[Exercise 1](#ex1)  [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)   [7](#ex7)   \n",
    "\n",
    "[Novel contribution](#novel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** For this script we will use a localizer dataset from [Kim et al. (2017)](https://doi.org/10.1523/JNEUROSCI.3272-16.2017) again. Just to recap: The localizer consisted of 3 runs with 5 blocks of each category (faces, scenes and objects) per run. Each block was presented for 15s. Within a block, a stimulus was presented every 1.5s (1 TR). Between blocks, there was 15s (10 TRs) of fixation. Each run was 310 TRs. In the matlab stimulus file, the first row codes for the stimulus category for each trial (1 = Faces, 2 = Scenes, 3 = Objects). The 3rd row contains the time (in seconds, relative to the start of the run) when the stimulus was presented for each trial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1f7f9d75-833f-410f-8988-58c1618fa753"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# Import fMRI and general analysis libraries\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# %matplotlib notebook\n",
    "\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline \n",
    "%autosave 5\n",
    "sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})\n",
    "sns.set(palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have to import the functions of interest\n",
    "from utils import load_data, load_labels, label2TR, shift_timing, reshape_data, blockwise_sampling, decode, normalize\n",
    "from utils import vdc_data_dir, vdc_all_ROIs, vdc_label_dict, vdc_n_runs, vdc_hrf_lag, vdc_TR, vdc_TRs_run\n",
    "\n",
    "print('Here\\'re some constants, which is specific for VDC data:')\n",
    "print('data dir = %s' % (vdc_data_dir))\n",
    "print('ROIs = %s' % (vdc_all_ROIs))\n",
    "print('Labels = %s' % (vdc_label_dict))\n",
    "print('number of runs = %s' % (vdc_n_runs))\n",
    "print('1 TR = %.2f sec' % (vdc_TR))\n",
    "print('HRF lag = %.2f sec' % (vdc_hrf_lag))\n",
    "print('num TRs per run = %d' % (vdc_TRs_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data <a id=\"load-data\"></a>\n",
    "\n",
    "Load the data for one participant using the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = 1\n",
    "mask_name = ''\n",
    "\n",
    "# Specify the subject name\n",
    "sub = 'sub-%.2d' % (sub_id)\n",
    "# Convert the shift into TRs\n",
    "shift_size = int(vdc_hrf_lag / vdc_TR)  \n",
    "\n",
    "# Load subject labels\n",
    "stim_label_allruns = load_labels(vdc_data_dir, sub)\n",
    "\n",
    "# Load run_ids\n",
    "run_ids = stim_label_allruns[5,:] - 1 \n",
    "\n",
    "# Load the fMRI data using a whole-brain mask\n",
    "epi_mask_data_all, _ = load_data(\n",
    "    directory=vdc_data_dir, subject_name=sub, mask_name=mask_name, zscore_data=True\n",
    ")\n",
    "\n",
    "# This can differ per participant\n",
    "print(sub, '= TRs: ', epi_mask_data_all.shape[1], '; Voxels: ', epi_mask_data_all.shape[0])\n",
    "TRs_run = int(epi_mask_data_all.shape[1] / vdc_n_runs)\n",
    "\n",
    "# Convert the timing into TR indexes\n",
    "stim_label_TR = label2TR(stim_label_allruns, vdc_n_runs, vdc_TR, TRs_run)\n",
    "\n",
    "# Shift the data some amount\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)\n",
    "\n",
    "# Perform the reshaping of the data\n",
    "bold_data, labels = reshape_data(stim_label_TR_shifted, epi_mask_data_all)\n",
    "\n",
    "# Down sample the data to be blockwise rather than trialwise\n",
    "bold_data, labels, run_ids = blockwise_sampling(bold_data, labels, run_ids)\n",
    "\n",
    "# Normalize within each run\n",
    "bold_normalized = normalize(bold_data, run_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Circular Inference: How to avoid double dipping <a id=\"double_dipping\"></a>\n",
    "\n",
    "The `GridSearchCV` method that you will learn about below makes it easy (though not guaranteed) to avoid double dipping. In previous exercises we examined cases where double dipping is clear (e.g., training on all of the data and testing on a subset). However, double dipping can be a lot more subtle and hard to detect, for example in situations where you perform feature selection on the entire dataset before classification (as in last week's notebook).\n",
    "\n",
    "We now examine some cases of double dipping again. This is a critically important issue for doing fMRI analysis correctly and for obtaining real results. We would like to emphasize through these examples:\n",
    "> 1. Whenever possible, never look at your test data before building your model.\n",
    "> 2. If you do build your model using test data, verify your model on random noise. Your model should report chance level performance. If not, something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Error: Voxel selection on all the data<a id=\"example-dd-vox-sel\"></a>\n",
    "\n",
    "Below we work through an exercise of a common type of double dipping in which we perform voxel selection on all of our data before splitting it into a training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = PredefinedSplit(run_ids)\n",
    "clf_score = np.array([])\n",
    "for train, test in sp.split():\n",
    "    \n",
    "    # Do voxel selection on all voxels\n",
    "    selected_voxels = SelectKBest(f_classif,k=100).fit(bold_normalized, labels)\n",
    "    \n",
    "    # Pull out the sample data\n",
    "    train_data = bold_normalized[train, :]\n",
    "    test_data = bold_normalized[test, :]\n",
    "\n",
    "    # Train and test the classifier\n",
    "    classifier = SVC(kernel=\"linear\", C=1)\n",
    "    clf = classifier.fit(selected_voxels.transform(train_data), labels[train])\n",
    "    score = clf.score(selected_voxels.transform(test_data), labels[test])\n",
    "    clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "print('Classification accuracy:', clf_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test: Verify procedure on random (permuted) labels<a id=\"example-dd-random\"></a>\n",
    "\n",
    "One way to check if the procedure is valid is to test it on random data. We can do this by randomly assigning labels to every block. This breaks the true connection between the labels and the brain data, meaning that there should be no basis for reliable classification. Next, we apply our method of selection and assess the classifier accuracy. If the classifier accuracy is above chance, we have done something wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10  # How many different permutations\n",
    "sp = PredefinedSplit(run_ids)\n",
    "clf_score = np.array([])\n",
    "\n",
    "for i in range(n_iters):\n",
    "    clf_score_i = np.array([])\n",
    "    permuted_labels = np.random.permutation(labels)\n",
    "    for train, test in sp.split():\n",
    "        # Do voxel selection on all voxels\n",
    "        selected_voxels = SelectKBest(f_classif,k=100).fit(bold_normalized, labels)\n",
    "\n",
    "        # Pull out the sample data\n",
    "        train_data = bold_normalized[train, :]\n",
    "        test_data = bold_normalized[test, :]\n",
    "\n",
    "        # Train and test the classifier\n",
    "        classifier = SVC(kernel=\"linear\", C=1)\n",
    "        clf = classifier.fit(selected_voxels.transform(bold_normalized), permuted_labels)\n",
    "        score = clf.score(selected_voxels.transform(test_data), permuted_labels[test])\n",
    "        clf_score_i = np.hstack((clf_score_i, score))\n",
    "    clf_score = np.hstack((clf_score, clf_score_i.mean()))\n",
    "        \n",
    "print ('Mean Classification across %d folds: %0.4f' % (n_iters, clf_score.mean()))\n",
    "print ('Standard Error: %0.4f'% sem(clf_score))\n",
    "print ('Chance level: %0.4f' % (1/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>We see above chance decoding accuracy! Something is wrong.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**<a id=\"ex1\"></a> Rewrite the above code to fix the concerns about double dipping and verify that the accuracy on test data is at chance level when the labels are randomly permuted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong> Sometimes you don't want to perform Leave-One-Run-Out </strong>\n",
    "<br>\n",
    "If we have different runs (or even a single run) but don't want to use them as the basis for your training/test splits (for instance because we think that participants are responding differently on later vs. earlier runs; or we have only one run in the experiment in which we show a long movie), we can run into double dipping issues. For example, if you only have one run, it can still be useful to z-score each voxel (over time) within that run. Without z-scoring, voxels may have wildly different scales due to scanner drift or other confounds, distorting the classifier. Hence we need to normalize within run but this could be considered double dipping because each run includes both training and test data.  In these circumstances, it may (or may not) be fine to z-score over the entire dataset. <strong>Always verify the model performance by randomizing the labels!</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eee6bfd7-e6b6-4860-8501-6a3799dba268"
    }
   },
   "source": [
    "## 3. Cross-Validation: Hyper-parameter selection and regularization <a id=\"cross_val\"></a>\n",
    "\n",
    "\n",
    "Each of the classifiers we have used so far has one or more \"hyper-parameters\" used to configure and optimize the model based on the data and our goals. Read [this Machine Learning Mastery Article](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) for an explanation of the distinction between hyper-parameters and parameters. For instance, regularized logistic regression has a \"penalty\" hyper-parameter, which determines how much to emphasize the weight regularizing expression (e.g., L2 norm) when training the model.\n",
    "\n",
    "**Exercise 2:** <a id=\"ex2\"></a> SVM has a \"cost\" ('C') hyper-parameter, a.k.a. soft-margin hyper-parameter. Look up and briefly describe what it means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Grid Search <a id=\"grid_search\"></a>\n",
    "\n",
    "\n",
    "We want to pick the best cost hyper-parameter for our dataset and to do this we will use cross-validation. Hyper-parameters are often, but not always, continuous variables. Each hyper-parameter can be considered as a dimension such that the set of hyper-parameters is a space to be searched for effective values. The `GridSearchCV` method in [scikit-learn](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) explores this space by dividing it up into a grid of values to be searched exhaustively. \n",
    "\n",
    "To give you an intuition for how grid search works, imagine trying to figure out what climate you find most comfortable. Let's say that there are two (hyper-)parameters that seem relevant: temperature and humidity. A given climate can be defined by the combination of values of these two parameters and you could report how comfortable you find this climate. A grid search would involve changing the value of each parameter with respect to the other in some fixed step size (e.g., 60 degrees and 50% humidity, 60 degrees and 60% humidity, 65 degrees and 60% humidity, etc.) and evaluating your preference for each combination.  \n",
    "\n",
    "Note that the number of steps and hyper-parameters to search is up to you. But be aware of combinatorial explosion: the granularity of the search (the smaller the steps) and the number of hyper-parameters considered increases the search time exponentially.\n",
    "\n",
    "`GridSearchCV` is an *extremely* useful tool for hyper-parameter optimization because it is very flexible. You can look at different values of a hyper-parameter, different [kernels](http://scikit-learn.org/stable/modules/svm.html), different training/test split sizes, etc. The input to the function is a dictionary where the key is the parameter of interest (the sides of the grid) and the values are the parameter increments to search over (the steps of the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to do a grid search over the SVM cost ('C') hyper-parameter (we call it a grid search now, even though only a single dimension is being searched over) and investigate the results. The output contains information about the best hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over different cost parameters (C)\n",
    "parameters = {'C':[0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel='linear'),\n",
    "    parameters,\n",
    "    cv=PredefinedSplit(run_ids),\n",
    "    return_train_score=True\n",
    ")\n",
    "clf.fit(bold_normalized, labels)\n",
    "\n",
    "# Print the results\n",
    "print('What is the best model:', clf.best_estimator_)  # What was the best classifier and cost?\n",
    "print('What is the score of the best model:',clf.best_score_)  # What was the best classification score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see more details from the cross validation? All the results are stored in the dictionary `cv_results_`. Let's took a look at some of the important metrics stored here. For more details you can look at the `cv_results_` method on [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "You can printout `cv_results_` directly or for a nicer look you can import it into a pandas dataframe and print it out. Each row corresponds to one parameter combination.\n",
    "\n",
    "([Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) is a widely used data processing and machine learning package. Some people love it more than the animal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ugly way\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicer way (using pandas)\n",
    "results = pd.DataFrame(clf.cv_results_)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to do some different types of cross-validation hyper-parameter tuning.\n",
    "\n",
    "**Exercise 3:**<a id=\"ex3\"></a> In machine learning, kernels are classes of algorithms that can be used to create a model. The (gaussian) radial basis function (RBF) kernel is very common in SVM classifiers. Look up and briefly describe what it does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over different C and gamma parameters of a radial basis kernel\n",
    "parameters = {'gamma':[10e-4, 10e-3, 10e-2], 'C':[10e-3, 10e0, 10e3]}\n",
    "clf = GridSearchCV(SVC(kernel='rbf'),\n",
    "                   parameters,\n",
    "                   cv=PredefinedSplit(run_ids))\n",
    "clf.fit(bold_normalized, labels)\n",
    "print(clf.best_estimator_)  # What was the best classifier and parameters?\n",
    "print(clf.best_score_)  # What was the best classification score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**<a id=\"ex4\"></a>  When would linear SVM be expected to outperform other kernels and why? Answer this question and run an analysis in which you compare linear, polynomial, and RBF kernels for SVM using GridSearchCV. This doesn't mean you run three separate GridSearchCV calls, this mean you should use these kernels as different hyper-parameters (as well as fitting C and gamma).\n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Regularization Example: L2 vs. L1 <a id=\"reg\"></a>\n",
    "\n",
    "Regularization is a technique that helps to reduce overfitting by assigning a penalty to the weights formed by the model. One common classifier used is [logistic regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc). There are different regularization options for logistic regression, such as the L1 vs. L2 penalty. An L1 penalty penalizes the sum of the absolute values of the weights whereas an L2 (also called Euclidean) penalty penalizes the sum of the squares of the weights. The L1 penalty leads to a sparser set of weights, with some high and the rest close to zero. The L2 penalty results in weights having very small values. A more detailed explanation of (L2 and L1) regularization can be found [here](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/).\n",
    "\n",
    "Below, we compare the L1 and L2 penalty for logistic regression. For each of the penalty types, we run 3 folds and compute the correlation of weights across folds. If the weights on each voxel are similar across folds then that can be thought of as a stable model. A higher correlation means a more stable model. L1 penalties can result in better performance on any given fold, but the weights are less stable across folds compared to L2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare L1 and L2 regularization in logistic regression\n",
    "\n",
    "# Decode with L1 regularization\n",
    "logreg_l1 = LogisticRegression(penalty='l1')\n",
    "model_l1, score_l1 = decode(bold_normalized, labels, run_ids, logreg_l1)\n",
    "print('Accuracy with L1 penalty: ', score_l1)\n",
    "\n",
    "# Decode with L2 regularization\n",
    "logreg_l2 = LogisticRegression(penalty='l2')\n",
    "model_l2, score_l2 = decode(bold_normalized, labels, run_ids, logreg_l2)\n",
    "print('Accuracy with L2 penalty: ', score_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the weights for the 3 folds of the different types of regularization\n",
    "wts_l1 = np.stack([(model_l1[i].coef_).flatten() for i in range(len(model_l1))])\n",
    "wts_l2 = np.stack([(model_l2[i].coef_).flatten() for i in range(len(model_l2))])\n",
    "\n",
    "# Correlate the weights across each run with the other runs\n",
    "corr_l1 = np.corrcoef(wts_l1)\n",
    "corr_l2 = np.corrcoef(wts_l2)\n",
    "\n",
    "# Plot the correlations across the folds\n",
    "f, ax = plt.subplots(1,2, figsize = (12, 5))\n",
    "ax[0].set_title('L1, corr mean: %0.4f' % np.mean(corr_l1[np.triu(corr_l1, 1) > 0]))\n",
    "sns.heatmap(corr_l1, ax=ax[0], vmin=0, vmax=1)\n",
    "ax[1].set_title('L2, corr mean: %0.4f' % np.mean(corr_l2[np.triu(corr_l2, 1) > 0]))\n",
    "sns.heatmap(corr_l2, ax=ax[1], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**:<a id=\"ex5\"></a>  Why does L2 lead to more stable weights across folds? (Hint: Consider how L1 and L2 penalties would affect the weights assigned to two or more voxels that carry highly corrrelated information.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "You might be tempted to run grid search CV, find the best result, and then report that result in your paper. Unfortunately, that is big-time double-dipping (since you are using the test data to decide how to analyze your data). The right way to handle this issue is nested cross-validation, described below: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Nested Cross-validation: Hyper-parameter selection <a id='nested_cross_val'></a>   \n",
    "\n",
    "\n",
    "When we are writing a classification pipeline, nested cross validation can be very useful. As the name suggests, this procedure nests a second cross-validation within folds of the first cross validation. As before, we will divide data into training and test sets (outer loop), but additionally will divide the training set itself in order to set the hyper-parameters into training and validation sets (inner loop).\n",
    "\n",
    "Thus, on each split we now have a training (inner), validation (inner), and test (outer) dataset; we will use leave-one-run-out for the validation set in the inner loop. Within the inner loop we train the model and find the optimal hyper-parameters (i.e., that have the highest performance when tested on the validation data). The typical practice is to then retrain your model with these hyper-parameters on both the training AND validation datasets and then evaluate on your held-out test dataset to get a score.\n",
    "\n",
    "![image](https://i.stack.imgur.com/vh1sZ.png)\n",
    "\n",
    "This is turtles all the way down, you could have any number of inner loops. However, you will run into data issues quickly (not enough data for training) and you will also run the risk of over-fitting your data: you will find the optimal parameters for a small set of your data but this might not generalize to the rest of your data. For more description and a good summary of what you have learnt so far then check [here](http://www.predictiveanalyticsworld.com/patimes/nested-cross-validation-simple-cross-validation-isnt-enough/8952/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the training, validation, and testing set (the indexes that belong to each group)\n",
    "\n",
    "# Outer loop:\n",
    "# Split training (including validation) and testing set\n",
    "sp = PredefinedSplit(run_ids)\n",
    "for outer_idx, (train, test) in enumerate(sp.split()):\n",
    "    train_run_ids = run_ids[train]\n",
    "    print ('Outer loop % d:' % outer_idx)\n",
    "    print ('Testing: ')\n",
    "    print (test)\n",
    "    \n",
    "    # Inner loop (implicit, in GridSearchCV):\n",
    "    # split training and validation set\n",
    "    sp_train = PredefinedSplit(train_run_ids)\n",
    "    for inner_idx, (train_inner, val) in enumerate(sp_train.split()):\n",
    "        print ('Inner loop %d:' % inner_idx)\n",
    "        print ('Training: ')\n",
    "        print (train[train_inner])\n",
    "        print ('Validation: ')\n",
    "        print (train[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nested cross-validation using one subject and logistic regression\n",
    "\n",
    "# Outer loop:\n",
    "# Split training (including validation) and testing set\n",
    "sp = PredefinedSplit(run_ids)\n",
    "clf_score = np.array([])\n",
    "C_best = []\n",
    "for train, test in sp.split():\n",
    "    # Pull out the sample data\n",
    "    train_run_ids = run_ids[train]\n",
    "    train_data = bold_normalized[train, :]\n",
    "    test_data = bold_normalized[test, :]\n",
    "    train_label = labels[train]\n",
    "    test_label = labels[test]\n",
    "    \n",
    "    # Inner loop (implicit, in GridSearchCV):\n",
    "    # Split training and validation set\n",
    "    sp_train = PredefinedSplit(train_run_ids)\n",
    "    \n",
    "    # Search over different regularization parameters: smaller values specify stronger regularization.\n",
    "    parameters = {'C':[0.01, 0.1, 1, 10]}\n",
    "    inner_clf = GridSearchCV(\n",
    "        LogisticRegression(penalty='l2'),\n",
    "        parameters,\n",
    "        cv=sp_train,\n",
    "        return_train_score=True)\n",
    "    inner_clf.fit(train_data, train_label)\n",
    "    \n",
    "    # Find the best hyperparameter\n",
    "    C_best_i = inner_clf.best_params_['C']\n",
    "    C_best.append(C_best_i)\n",
    "    \n",
    "    # Train the classifier with the best hyperparameter using training and validation set\n",
    "    classifier = LogisticRegression(penalty='l2', C=C_best_i)\n",
    "    clf = classifier.fit(train_data, train_label)\n",
    "    \n",
    "    # Test the classifier\n",
    "    score = clf.score(test_data, test_label)\n",
    "    clf_score = np.hstack((clf_score, score))\n",
    "    \n",
    "print ('Inner loop classification accuracy:', clf_score)\n",
    "print ('Best cost value:', C_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**<a id=\"ex6\"></a> Set up a nested cross validation loop for the first 3 subjects using SVM with a linear kernel. Show the best C in each fold for each subject and both the average and standard error of classification accuracy across runs for each subject. In this loop you will perform hyper-parameter cross validation on a training dataset (which itself will be split into training and validation) and then score these optimized hyper-parameters on the test dataset. \n",
    "Things to watch out for: \n",
    "- Be careful not to use hyper-parameter optimization (e.g., with `GridSearchCV`) in both inner and outer loops of nested cross-validation\n",
    "- As always: If in doubt, check the [scikit-learn documentation](http://scikit-learn.org/stable/index.html) or [StackExchange Community](https://stackexchange.com/) for help\n",
    "- Running nested cross validation will take a couple of minutes. Grab a snack.\n",
    "- Use different variable names than the ones used above (such as `bold_normalized`, `labels` and `run_ids`) since we will still be using those data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Pipeline <a id=\"pipeline\"></a>\n",
    "\n",
    "In a previous notebook we had introduced the scikit-learn method, [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), that simplified running a sequence steps in an automated fashion. We will now use the pipeline to do feature selection and cross-validation. Below we create a pipeline with the following steps:   \n",
    ">Use PCA and choose the best option from a set of dimensions.  \n",
    ">Choose the best cost hyperparameter value for an SVM.\n",
    "\n",
    "It is then really easy to do cross validation at different levels of this pipeline.\n",
    "\n",
    "The steps below are based on [this example in scikit-learn](http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html#illustration-of-pipeline-and-gridsearchcv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "304fd1c3-80aa-4829-a36a-40c45dde562a"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the pipeline\n",
    "pipe = Pipeline([\n",
    "#         ('scale', preprocessing.StandardScaler()), # Could be part of our pipeline but we already have done it\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('classify', SVC(kernel=\"linear\")),\n",
    "    ])\n",
    "\n",
    "# PCA dimensions\n",
    "component_steps = [20, 30]\n",
    "\n",
    "# Classifier cost options\n",
    "c_steps = [10e-1, 10e0, 10e1, 10e2]\n",
    "\n",
    "# Build the grid search dictionary\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7)], \n",
    "        'reduce_dim__n_components': component_steps,\n",
    "        'classify__C': c_steps,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put it all together and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelization parameter, will return to this later...\n",
    "n_jobs=1\n",
    "\n",
    "clf_pipe = GridSearchCV(pipe,\n",
    "                        cv=PredefinedSplit(run_ids),\n",
    "                        n_jobs=n_jobs,\n",
    "                        param_grid=param_grid,\n",
    "                        return_train_score=True\n",
    "                       )\n",
    "clf_pipe.fit(bold_normalized, labels)  # run the pipeline\n",
    "\n",
    "print(clf_pipe.best_estimator_)  # What was the best classifier and parameters?\n",
    "print()  # easy way to output a blank line to structure your output\n",
    "print(clf_pipe.best_score_)  # What was the best classification score?\n",
    "print()\n",
    "\n",
    "# sort results with declining mean test score\n",
    "cv_results = pd.DataFrame(clf_pipe.cv_results_)\n",
    "print(cv_results.sort_values(by='mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:**<a id=\"ex7\"></a> Build a pipeline that takes the following steps:\n",
    "\n",
    "1. Use the ANOVA method (SelectKBest and f_classif) for voxel selection. Grid search over the k value.\n",
    "2. Grid search over the linear and RBF SVM kernel.\n",
    "\n",
    "Run this pipeline for at least 5 subjects and present your average results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:**<a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a> \n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 02/2018  \n",
    "T. Meissner minor edits  \n",
    "H. Zhang added random label and regularization exercises, change to PredefinedSplit, use normalized data, add solutions, other edits.  \n",
    "M. Kumar re-organized the sections and added section context.  \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
    "C. Ellis incorporated comments from cmhn-s19"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
