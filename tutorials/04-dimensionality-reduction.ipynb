{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "[Contributions](#contributions)\n",
    "\n",
    "fMRI analysis often has a dimensionality problem: we get approximately 100,000 voxels (i.e., features) per volume, but only 100s of time points or trials (i.e., examples). This makes it very hard for machine learning algorithms to model how each voxel contributes. For more general information on this problem, also dubbed the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), see [these slides from the Texas A&M University Computer Science and Engineering Department](http://courses.cs.tamu.edu/choe/11spring/633/lectures/slide08.pdf). For a neuroimaging-specific view on the curse of dimensionality, you might want to take a look at [Mwangi et al.'s Neuroinformatics review from 2014](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4040248/).\n",
    "\n",
    "In this notebook we are going to learn various methods that can help us reduce the dimensionality of fMRI data.\n",
    "\n",
    "## Goal of this script\n",
    "1. Learn to compute the covariance of a dataset.  \n",
    "2. Reduce the feature space using principal component analysis (PCA).  \n",
    "3. Interpret the meaning of PCA components.  \n",
    "4. Perform feature selection using cross-validation.  \n",
    "\n",
    "## Pre-requisties\n",
    "You should be familiar with the functions in the data loading and classification notebooks.\n",
    "\n",
    "## Table of Contents\n",
    "[1. Load the data](#load-data)  \n",
    "\n",
    "[2. Covariance](#covariance)  \n",
    "\n",
    "[3. PCA](#pca)  \n",
    ">[3.1 Plot PCA](#plot_pca)  \n",
    ">[3.2 Scree Plots](#scree)  \n",
    ">[3.3 Interpreting Components](#cog-relevance)  \n",
    ">[3.4 Normalization](#pca-norm)  \n",
    ">[3.5  PCA dimensionality reduction and classification](#wb-pca-class)  \n",
    "\n",
    "[4. Feature Selection](#feat)  \n",
    ">[4.1 Feature Selection: Pipelines](#pipeline)  \n",
    ">[4.2 Feature Selection: Univariate](#univariate)   \n",
    "\n",
    "Exercises\n",
    ">[Exercise 1](#ex1)  [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)  [7](#ex7)  [8](#ex8)   [9](#ex9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** For this script we will use a localizer dataset from [Kim et al. (2017)](https://doi.org/10.1523/JNEUROSCI.3272-16.2017) again. Just to recap: The localizer consisted of 3 runs with 5 blocks of each category (faces, scenes and objects) per run. Each block was presented for 15s. Within a block, a stimulus was presented every 1.5s (1 TR). Between blocks, there was 15s (10 TRs) of fixation. Each run was 310 TRs. In the matlab stimulus file, the first row codes for the stimulus category for each trial (1 = Faces, 2 = Scenes, 3 = Objects). The 3rd row contains the time (in seconds, relative to the start of the run) when the stimulus was presented for each trial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "# Import neuroimaging, analysis and general libraries\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, RFECV, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "%autosave 5\n",
    "sns.set(style = 'white', context='poster', rc={'lines.linewidth': 2.5})\n",
    "sns.set(palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some helper functions\n",
    "from utils import load_labels, load_data, blockwise_sampling, label2TR, shift_timing, reshape_data\n",
    "from utils import normalize, decode\n",
    "# load some constants\n",
    "from utils import vdc_data_dir, vdc_all_ROIs, vdc_label_dict, vdc_n_runs, vdc_hrf_lag, vdc_TR, vdc_TRs_run\n",
    "\n",
    "print('Here\\'re some constants, which is specific for VDC data:')\n",
    "print('data dir = %s' % (vdc_data_dir))\n",
    "print('ROIs = %s' % (vdc_all_ROIs))\n",
    "print('Labels = %s' % (vdc_label_dict))\n",
    "print('number of runs = %s' % (vdc_n_runs))\n",
    "print('1 TR = %.2f sec' % (vdc_TR))\n",
    "print('HRF lag = %.2f sec' % (vdc_hrf_lag))\n",
    "print('num TRs per run = %d' % (vdc_TRs_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data <a id=\"load-data\"></a>\n",
    "\n",
    "Load the data for one participant using these helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = 1\n",
    "mask_name = ''\n",
    "\n",
    "# Specify the subject name\n",
    "sub = 'sub-%.2d' % (sub_id)\n",
    "# Convert the shift into TRs\n",
    "shift_size = int(vdc_hrf_lag / vdc_TR)  \n",
    "\n",
    "# Load subject labels\n",
    "stim_label_allruns = load_labels(vdc_data_dir, sub)\n",
    "\n",
    "# Load run_ids\n",
    "run_ids_raw = stim_label_allruns[5,:] - 1 \n",
    "\n",
    "# Load the fMRI data using a mask\n",
    "epi_mask_data_all = load_data(vdc_data_dir, sub, mask_name=mask_name)[0]\n",
    "\n",
    "# This can differ per participant\n",
    "print(sub, '= TRs: ', epi_mask_data_all.shape[1], '; Voxels: ', epi_mask_data_all.shape[0])\n",
    "TRs_run = int(epi_mask_data_all.shape[1] / vdc_n_runs)\n",
    "\n",
    "# Convert the timing into TR indexes\n",
    "stim_label_TR = label2TR(stim_label_allruns, vdc_n_runs, vdc_TR, TRs_run)\n",
    "\n",
    "# Shift the data some amount\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)\n",
    "\n",
    "# Perform the reshaping of the data\n",
    "bold_data_raw, labels_raw = reshape_data(stim_label_TR_shifted, epi_mask_data_all)\n",
    "\n",
    "# Normalize raw data within each run\n",
    "bold_normalized_raw = normalize(bold_data_raw, run_ids_raw)\n",
    "\n",
    "# Down sample the data to be blockwise rather than trialwise. \n",
    "#We'll use the blockwise data for all the \n",
    "bold_data, labels, run_ids = blockwise_sampling(bold_data_raw, labels_raw, run_ids_raw)\n",
    "\n",
    "# Normalize blockwise data within each run\n",
    "bold_normalized = normalize(bold_data, run_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Block Averaging</strong>\n",
    "<br>\n",
    "Previously, we have been using data from each trial. Within each block, the voxel activity is correlated across trials. Thus, it is common (and probably better) to take the average value of the activity within a block as your observation in decoding analyses in order to avoid concerns about non-independence. Mean values of activity or beta coefficients (from GLM) are commonly used in the literature.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** We introduce a simple kind of debugging here, as we print both the number of expected and resampled blocks (resampled refers to the conversion from trialwise data to blockwise data). Thus, if something went wrong, we would be able to spot it the output. Learn about more ways of debugging your code by using assertions [here](https://wiki.python.org/moin/UsingAssertionsEffectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariance <a id=\"covariance\"></a>\n",
    "\n",
    "As a precursor to understanding dimensionality reduction techniques, we need to learn how to compute the covariance matrix because it is often used in these methods.  \n",
    "\n",
    "We are going to work with whole-brain data this time. You might have memory issues but this is an important problem to grapple with. There are nearly 1 million voxels in every volume we acquire, of which about 15% are in the brain. The data matrix of >100,000 voxels and <1000 time points is very large, making any computations on all of this data very intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance of two variables is calculated as follows: $$ Cov(X,Y) = \\frac{\\sum_{1}^{N}(X-\\bar{X})(Y-\\bar{Y})}{(N-1)}$$\n",
    "where $\\mbox{  }  \\bar{X} = mean(X), \\mbox{  } \\bar{Y} = mean(Y), \\mbox{  } N = \\mbox{number of samples } $\n",
    "\n",
    "In fMRI, X and Y could be time-series data for two voxels (two columns in our time by voxels data matrix) or the pattern across voxels for two different time points (two rows in the data matrix). The choice of vectors depends on the application.\n",
    "\n",
    "**Exercise 1:** <a id=\"ex1\"></a> Compute the covariance between two blocks (i.e., their averaged patterns across voxels). The steps to do this are outlined below. You could just use a function but we want you to code the individual steps as described (refer [here]( https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.cov.html) for additional help)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "# Compute the mean of one row of the block-averaged bold data called: X\n",
    "\n",
    "# Compute the mean of any other row of the block-averaged bold data called: Y\n",
    "\n",
    "# Compute the differences of individual voxel values in these rows from the corresponding mean for X or Y.\n",
    "\n",
    "# Compute the pointwise product of the difference vectors across the two rows.\n",
    "\n",
    "# Sum over the products of the differences.\n",
    "\n",
    "# Complete the covariance calculation with these values.\n",
    "\n",
    "# Compare your result to the answer obtained with np.cov(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is dependent on the unit and scale of the measurement. Its value is thus not easily interpretable or comparable across datasets -- e.g. is there a strong relationship between X and Y if the covariance is 200 as compared to 2 or 2000?\n",
    "\n",
    "Correlation solves this problem by normalizing the range of the covariance from -1 to +1.\n",
    "\n",
    "$$ Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{\\frac{\\sum_{1}^{N}(X-\\bar{X})^2}{(N-1)}}\\sqrt{\\frac{\\sum_{1}^{N}(Y-\\bar{Y})^2}{(N-1)}}}$$\n",
    "\n",
    "**Exercise 2:** <a id=\"ex2\"></a> Compute the correlation between all pairs of blocks manually (one pair at a time) and compare the result with a numpy function that calculates the block-by-block correlation matrix in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation manually\n",
    "\n",
    "# Now with a function  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: <a id=\"ex3\"></a> Now compute the covariance between time-series across pairs of voxels (using the np.cov). Perform this compution on a group of 100 voxels in order to make a voxel-by-voxel covariance matrix in one step (no `for` loops allowed). Make sure the output is the correct shape (100, 100).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "\n",
    "# Subselect 100 voxels from bold_data into a matrix.\n",
    "\n",
    "# Use np.cov() to compute the covariance of this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA <a id=\"pca\"></a>\n",
    "\n",
    "We will use principal component analysis (PCA) to **reduce the dimensionality** of the data. Some voxels may contain correlated information or no information and so the original voxel-dimensional data matrix (time-by-voxels) can be projected into a lower-dimensional \"component\" matrix space (time-by-component) without losing much information.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1200/1*Iri_LDMXuz2Qac-8KPeESA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use the PCA function in scikit-learn to reduce the dimensionality of the data\n",
    "# The number of components was chosen arbitrarily.\n",
    "pca = PCA(n_components=20)\n",
    "bold_pca = pca.fit_transform(bold_data)\n",
    "\n",
    "print('Original data shape:', bold_data.shape)\n",
    "print('PCA data shape:', bold_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plot PCA <a id=\"plot_pca\"></a>\n",
    "\n",
    "Let's visualize the variance in the data along different component dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting plotting parameter\n",
    "n_bins=75\n",
    "\n",
    "# Plot\n",
    "n_plots = 4\n",
    "components_to_plot = [0,1,2,19]\n",
    "sns.set(font_scale=1.5)\n",
    "f, axes = plt.subplots(1, n_plots, figsize=(14, 14/n_plots))\n",
    "st=f.suptitle(\"Figure 3.1. Histogram of values for each PC dimension \", fontsize=\"x-large\")\n",
    "\n",
    "for i in range(n_plots): \n",
    "    axes[i].hist(bold_pca[:, components_to_plot[i]], \n",
    "                 bins=n_bins)\n",
    "    # mark the plots \n",
    "    axes[i].set_title('PC Dimension %d'%(components_to_plot[i]+1))\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_xlabel('Value')    \n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])    \n",
    "\n",
    "f.tight_layout()\n",
    "st.set_y(0.95)\n",
    "f.subplots_adjust(top=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the relationship between variances across pairs of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the low dim representation of the bold data\n",
    "\"\"\"\n",
    "# Setting plotting parameters\n",
    "alpha_val = .8\n",
    "cur_pals = sns.color_palette('colorblind', n_colors=vdc_n_runs)\n",
    "\n",
    "# Plot\n",
    "n_plots = 3 \n",
    "f, axes = plt.subplots(1, n_plots, figsize=(14,5))\n",
    "st=f.suptitle(\"Figure 3.2. Scatter plots comparing PCA dimensions \", fontsize=\"x-large\")\n",
    "\n",
    "# plot data\n",
    "axes[0].scatter(bold_pca[:, 0], bold_pca[:, 1], \n",
    "                alpha=alpha_val, marker='.', color = 'k')\n",
    "axes[1].scatter(bold_pca[:, 2], bold_pca[:, 3], \n",
    "                alpha=alpha_val, marker='.', color = 'k')\n",
    "axes[2].scatter(bold_pca[:, 18], bold_pca[:, 19], \n",
    "                alpha=alpha_val, marker='.', color = 'k')\n",
    "\n",
    "axes[0].set_title('PCA Dimensions\\n1 x 2')\n",
    "axes[1].set_title('PCA Dimensions\\n3 x 4')\n",
    "axes[2].set_title('PCA Dimensions\\n18 x 19')\n",
    "\n",
    "# modifications that are common to all plots \n",
    "for i in range(n_plots): \n",
    "    axes[i].axis('equal')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "f.tight_layout()\n",
    "st.set_y(0.95)\n",
    "f.subplots_adjust(top=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scree plots <a id=\"scree\"></a>\n",
    "\n",
    "A [\"scree\" plot](https://www.theanalysisfactor.com/factor-analysis-how-many-factors/) can depict the amount of variance in the original data that is explained by each component.\n",
    "\n",
    "**Exercise 4:** <a id=\"ex4\"></a> Make a scree plot for the PCA above. How many components would be sufficient to account for most of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Interpreting Components  <a id=\"cog-relevance\"></a>\n",
    "\n",
    "From the previous plot of the first and second PCA dimension, you can see you have three clusters. You might assume that they correspond to faces, scenes, and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:**  <a id=\"ex5\"></a>Determine what the three clusters correspond to. First, create a new scatter plot of these two components and mark (e.g., in different symbols or colors) each point on the plot by visual category. Then, create a second scatter plot with points labeled in a way that better corresponds to the clusters (complete this exercise before reading further). (Hint: What else was there three of?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Normalization <a id=\"pca-norm\"></a>\n",
    "\n",
    "We ran the PCA analysis without normalizing the data.\n",
    "\n",
    "**Exercise 6:**<a id=\"ex6\"></a> Using the variable `bold_normalized` re-compute the PCA (components=20). Plot the results with a scatter plot like **Figure 3.2**. What was the effect of normalization and why is this useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 PCA dimensionality reduction and classification <a id=\"wb-pca-class\"></a>\n",
    "As mentioned earlier, we use PCA to reduce the dimensionality of the data and thus minimize the 'curse of dimensionality'. Below we explore how PCA affects classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a basic n-fold classification\n",
    "\n",
    "# Get baseline, whole-brain decoding accuracy without PCA\n",
    "print('Baseline classification')\n",
    "print('Original size: ', bold_normalized.shape)\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "\n",
    "start = time()\n",
    "models, scores = decode(bold_normalized, labels, run_ids, svc)\n",
    "end = time()\n",
    "print('Accuracy: ', scores)\n",
    "print('Run time: %0.4fs' %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on data in component space \n",
    "pca = PCA(n_components=20)\n",
    "bold_pca_normalized = pca.fit_transform(bold_normalized)\n",
    "print('PCA (c=%d) classification' % bold_pca_normalized.shape[1])\n",
    "print('New size after PCA: ', bold_pca_normalized.shape)\n",
    "\n",
    "start = time()\n",
    "models_pca, scores_pca = decode(bold_pca_normalized, labels, run_ids, svc)\n",
    "end = time()\n",
    "print('Accuracy: ', scores_pca)\n",
    "print('Run time: %0.4fs' %(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case PCA does not improve decoding accuracy. However, note that similar performance was achieved with 20 vs. 177,314 features, that the analysis ran 500x faster, and that the resulting model is likely to generalize better to new data (e.g., from a different subject)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** <a id=\"ex7\"></a> We used an arbitrary number of components. How does decoding accuracy change with more or less components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 The PCA Challenge <a id=\"pca-challenge\"></a>\n",
    "\n",
    "**Exercise 8:** <a id=\"ex7\"></a>Given that some of the early PCA dimensions may not be cognitively relevant, determine the smallest number of PCA components from which you can get the highest decoding accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection using cross-validation <a id=\"feat\"></a>\n",
    "\n",
    "When we took a few PCA components instead of all voxels, we were performing feature selection. Feature selection is used to reduce noise and increase computational speed. However, a problem with the approach above is that feature selection is applied to all data (prior to division into training and test sets) and is thus a kind of double dipping.\n",
    "\n",
    "A better way to select features is during cross-validation. In this case, feature selection is only performed on the training set, and the same features are used on the test data. This way the classifier never sees the test data during training.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Aside:</strong>  While doing PCA on the whole dataset violates the principle of “never touch your test data during training”, researchers have sometimes used this approach, justifying it on the grounds that — while PCA is using the fMRI data from the test set — it is not peeking at the class labels from the test set, and thus it will not bias classification accuracy. Is this OK? It’s difficult to say *a priori*. It is always safer to completely avoid touching the test data during training, so you should do this if at all possible. If you aren’t sure what problems might emerge from a particular analysis method, a good check of your method is to test on random noise; when you do this, classification should not exceed chance (if it does, you have a problem…)\n",
    "</div>\n",
    "\n",
    "We will perform feature selection during cross-validation in this section. The `Pipelines` method in scikit-learn provides an easy interface to perform these steps and we will use it extensively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Pipelines: Feature selection with cross-validation  <a id=\"pipeline\"></a>\n",
    "\n",
    "The scikit-learn has a method, [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), that simplifies running a sequence of steps in an automated fashion. Below we create a pipeline with the following steps:\n",
    "  \n",
    ">Perform dimensionality reduction.  \n",
    ">Run an SVM.\n",
    "\n",
    "To do this systematically during cross-validation, we will embed `Pipeline` in the `cross_validate` method in scikit-learn.\n",
    "\n",
    "The steps below are based on [this example in scikit-learn](http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html#illustration-of-pipeline-and-gridsearchcv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \n",
    "# Set up the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA(n_components=20)),\n",
    "    ('classify', SVC(kernel=\"linear\", C=1)),\n",
    "])\n",
    "\n",
    "# Run the pipeline with cross-validation\n",
    "ps = PredefinedSplit(run_ids) # Provides train/test indices to split data in train/test sets\n",
    "clf_pipe = cross_validate(\n",
    "    pipe,bold_normalized,labels,cv=ps,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Print results from this dimensionality reduction technique\n",
    "print(clf_pipe)\n",
    "print (\"Average Testing Accuracy: %0.2f\" % (np.mean(clf_pipe['test_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the data indices that were used for training and testing. Ensure that they are different for each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train/test split\n",
    "for cv_idx ,(train_index, test_index) in enumerate(ps.split(bold_normalized, labels)):\n",
    "    print('CV iteration: %s' % cv_idx)\n",
    "    print('Train_index: ')\n",
    "    print(train_index)\n",
    "    print('Test_index: ')\n",
    "    print(test_index)\n",
    "\n",
    "# Print results from this dimensionality reduction technique\n",
    "print(clf_pipe)\n",
    "print (\"Average Testing Accuracy: %0.2f\" % (np.mean(clf_pipe['test_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature selection: Univariate <a id=\"univariate\"></a>\n",
    "\n",
    "We can also use a variety of univariate methods to do feature selection in scikit-learn. One commonly used technique is to compute an ANOVA on the data and pick voxels with large F values. The F value measures the ratio of the variance between conditions (signal) to the variance within condition (noise). You can learn more about the ANOVA here:  [ANOVA F-value](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html). Note that implementing this completely different feature selection approach requires changing only one line in the pipeline, demonstrating the usefulness of this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** <a id=\"ex9\"></a> Implement the pipeline using ANOVA F-value (imported as `f_classif`) and the [`SelectKBest` method](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) pick the top 100 voxels with the highest F values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:** <a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization. This week we encourage you to implement a different feature selection [approach](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a>\n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook  02/2018  \n",
    "T. Meissner minor edits and added the ICA section  \n",
    "Q. Lu revise PCA plots, cb colors, code style improvement, leverage exisiting funcs  \n",
    "H. Zhang added pipeline section, use blockwise normalized data, other edits  \n",
    "M. Kumar enhanced section introductions.  \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
    "C. Ellis implemented comments from cmhn-s19"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
